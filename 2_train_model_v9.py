import pandas as pd
import numpy as np
import pandas_ta as ta
from xgboost import XGBClassifier
from catboost import CatBoostClassifier
from lightgbm import LGBMClassifier
from sklearn.ensemble import VotingClassifier
from sklearn.metrics import precision_score
from sklearn.model_selection import TimeSeriesSplit
import joblib
import os

# ==========================================
# 1. Ù‡Ù†Ø¯Ø³Ø© Ø§Ù„Ù…ÙŠØ²Ø§Øª (V9.1 - Ø§Ù„Ù…Ù†Ù‚Ø­Ø©)
# ==========================================
def feature_engineering_v9(df):
    data = df.copy()
    
    # 1. Ø¶Ø¨Ø· Ø§Ù„ÙÙ‡Ø±Ø³
    if not isinstance(data.index, pd.DatetimeIndex):
        if 'timestamp' in data.columns:
            data['timestamp'] = pd.to_datetime(data['timestamp'])
            data.set_index('timestamp', inplace=True)
            data.sort_index(inplace=True)
    
    # 2. Ø§Ù„Ø­Ø³Ø§Ø¨Ø§Øª Ø§Ù„Ø±ÙŠØ§Ø¶ÙŠØ©
    data['log_ret'] = np.log(data['close'] / data['close'].shift(1))
    
    # Garman-Klass Volatility
    data['GK_Vol'] = ((np.log(data['high'] / data['low'])**2) / 2) - \
                     (2 * np.log(2) - 1) * ((np.log(data['close'] / data['open'])**2))
    
    # 3. Ù…ÙŠØ²Ø§Øª Ø§Ù„Ø²Ø®Ù… ÙˆØ§Ù„Ø³ÙŠÙˆÙ„Ø©
    data['RSI'] = data.ta.rsi(length=14)
    data['ADX'] = data.ta.adx(length=14)['ADX_14']
    data['ATR'] = data.ta.atr(length=14)
    
    # VWAP Ù…Ø¹ Ø­Ù…Ø§ÙŠØ©
    data['vwap'] = data.ta.vwap()
    if data['vwap'].isnull().all():
         data['vwap'] = (data['high'] + data['low'] + data['close']) / 3
    data['dist_vwap'] = (data['close'] - data['vwap']) / (data['vwap'] + 1e-9)
    
    # 4. ÙÙ„ØªØ± Ø§Ù„ØªØ°Ø¨Ø°Ø¨ (Ø¬Ø¯ÙŠØ¯): Ù‡Ù„ Ø§Ù„Ø³ÙˆÙ‚ ÙŠØªØ­Ø±Ùƒ Ø£ØµÙ„Ø§Ù‹ØŸ
    # Ø³Ù†Ø³ØªØ®Ø¯Ù… Ù‡Ø°Ø§ Ù„Ø§Ø­Ù‚Ø§Ù‹ Ù„ØªØ¬Ø§Ù‡Ù„ Ø§Ù„ØªØ°Ø¨Ø°Ø¨ Ø§Ù„Ù…ÙŠØª
    data['Is_Choppy'] = np.where((data['ADX'] < 20) & (data['RSI'].between(40, 60)), 1, 0)

    data.dropna(inplace=True)
    return data

# ==========================================
# 2. ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ø£Ù‡Ø¯Ø§Ù (Ù…Ø®ÙÙ ÙˆØ£ÙƒØ«Ø± ÙˆØ§Ù‚Ø¹ÙŠØ©)
# ==========================================
def labeling_triple_barrier(df, horizon=24, vol_mult=1.0):
    # Ø²Ø¯Ù†Ø§ Ø§Ù„Ø£ÙÙ‚ Ø§Ù„Ø²Ù…Ù†ÙŠ Ù„Ù€ 24 Ø³Ø§Ø¹Ø© Ù„ÙƒÙŠ Ù†Ø¹Ø·ÙŠ Ø§Ù„ØµÙÙ‚Ø© Ù†ÙØ³Ø§Ù‹
    # Ø®ÙØ¶Ù†Ø§ Ù…Ø¶Ø§Ø¹Ù Ø§Ù„ØªÙ‚Ù„Ø¨ Ù„Ù€ 1.0 Ù„Ù†Ø¬Ø¹Ù„ Ø§Ù„Ø£Ù‡Ø¯Ø§Ù Ø£Ù‚Ø±Ø¨
    targets = []
    prices = df['close'].values
    atr = df['ATR'].values
    highs = df['high'].values
    lows = df['low'].values
    
    for i in range(len(df) - horizon):
        curr = prices[i]
        limit = atr[i] * vol_mult
        
        # Ø§Ù„Ù‡Ø¯Ù: 1.2 Ø¶Ø¹Ù Ø§Ù„Ù…Ø®Ø§Ø·Ø±Ø© (Ø£ÙƒØ«Ø± ÙˆØ§Ù‚Ø¹ÙŠØ© Ù…Ù† 1.8)
        tp = curr + (limit * 1.2) 
        sl = curr - (limit * 1.0)
        
        outcome = 0
        for j in range(1, horizon + 1):
            if i+j >= len(df): break
            h = highs[i+j]
            l = lows[i+j]
            
            if l <= sl:
                outcome = 0 # Ø¶Ø±Ø¨ Ø§Ù„ÙˆÙ‚Ù
                break
            if h >= tp:
                outcome = 1 # Ø¶Ø±Ø¨ Ø§Ù„Ù‡Ø¯Ù
                break
        targets.append(outcome)
    return targets

# ==========================================
# 3. Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ø°ÙƒÙŠ (Auto-Tuning)
# ==========================================
def train_brain_v9():
    print("ğŸ§  (V9.1 Stabilized) Ø¨Ø¯Ø¡ Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ù…Ø¹ Ø§Ù„ØªØµØ­ÙŠØ­ Ø§Ù„Ø°Ø§ØªÙŠ...")
    
    if not os.path.exists('data/btc_data_v9.csv'):
        print("âŒ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù…ÙÙ‚ÙˆØ¯Ø©!")
        return

    df = pd.read_csv('data/btc_data_v9.csv')
    df = feature_engineering_v9(df)
    
    print("ğŸ¯ ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ø£Ù‡Ø¯Ø§Ù (Easy Mode)...")
    targets = labeling_triple_barrier(df)
    
    df = df.iloc[:len(targets)].copy()
    df['Target'] = targets
    
    # Ø­Ø°Ù Ø£ÙˆÙ‚Ø§Øª "Ù…ÙˆØª Ø§Ù„Ø³ÙˆÙ‚" Ù…Ù† Ø§Ù„ØªØ¯Ø±ÙŠØ¨ (Ù„ØªÙ‚Ù„ÙŠÙ„ Ø§Ù„Ø¶Ø¬ÙŠØ¬)
    print(f"ğŸ“‰ Ù‚Ø¨Ù„ Ø§Ù„ØªÙ†Ø¸ÙŠÙ: {len(df)} Ø³Ø¬Ù„")
    df = df[df['Is_Choppy'] == 0]
    print(f"ğŸ“ˆ Ø¨Ø¹Ø¯ Ø¥Ø²Ø§Ù„Ø© Ø§Ù„ØªØ°Ø¨Ø°Ø¨ Ø§Ù„Ù…ÙŠØª: {len(df)} Ø³Ø¬Ù„")
    
    features = ['log_ret', 'GK_Vol', 'dist_vwap', 'RSI', 'ADX', 'ATR']
    if 'fng_value' in df.columns: features.append('fng_value')
    
    X = df[features]
    y = df['Target']
    
    # Ø·Ø¨Ø§Ø¹Ø© ØªÙˆØ§Ø²Ù† Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
    win_rate_base = (y.sum() / len(y)) * 100
    print(f"ğŸ“Š Ù†Ø³Ø¨Ø© Ø§Ù„ØµÙÙ‚Ø§Øª Ø§Ù„Ù†Ø§Ø¬Ø­Ø© ÙÙŠ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª: {win_rate_base:.2f}%")
    
    tscv = TimeSeriesSplit(n_splits=5)
    scores = []
    best_thresholds = []

    print("\nğŸ”¬ Ø§Ù„ØªØ­Ù‚Ù‚ Ø§Ù„Ø²Ù…Ù†ÙŠ ÙˆØ§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø§Ù„Ø¹ØªØ¨Ø© (Threshold Search):")
    for fold, (train_index, test_index) in enumerate(tscv.split(X)):
        X_train, X_test = X.iloc[train_index], X.iloc[test_index]
        y_train, y_test = y.iloc[train_index], y.iloc[test_index]
        
        # Ù…ÙˆØ§Ø²Ù†Ø©
        if len(y_train.unique()) < 2: continue
        ratio = np.sqrt((y_train==0).sum() / (y_train==1).sum())
        
        # Ù†Ù…Ø§Ø°Ø¬ Ø£Ø®Ù ÙˆØ£Ø³Ø±Ø¹ (Less Overfitting)
        clf1 = XGBClassifier(n_estimators=300, max_depth=4, learning_rate=0.03, scale_pos_weight=ratio, n_jobs=-1, random_state=42)
        clf2 = CatBoostClassifier(iterations=300, depth=4, learning_rate=0.03, auto_class_weights='SqrtBalanced', verbose=False, random_state=42)
        clf3 = LGBMClassifier(n_estimators=300, max_depth=4, learning_rate=0.03, class_weight='balanced', verbose=-1, random_state=42)
        
        ensemble = VotingClassifier(estimators=[('xgb', clf1), ('cat', clf2), ('lgbm', clf3)], voting='soft')
        ensemble.fit(X_train, y_train)
        
        # Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„Ø¹ØªØ¨Ø§Øª
        probs = ensemble.predict_proba(X_test)[:, 1]
        
        fold_best_prec = 0
        fold_best_thresh = 0.5
        fold_trades = 0
        
        # Ù†Ø¬Ø±Ø¨ Ø¹ØªØ¨Ø§Øª Ù…Ø®ØªÙ„ÙØ© Ù„ÙƒÙ„ ÙØªØ±Ø© Ø²Ù…Ù†ÙŠØ©
        for t in np.arange(0.5, 0.95, 0.05):
            preds = (probs >= t).astype(int)
            if preds.sum() > 5: # Ø´Ø±Ø· ÙˆØ¬ÙˆØ¯ 5 ØµÙÙ‚Ø§Øª Ø¹Ù„Ù‰ Ø§Ù„Ø£Ù‚Ù„
                p = precision_score(y_test, preds, zero_division=0)
                if p > fold_best_prec:
                    fold_best_prec = p
                    fold_best_thresh = t
                    fold_trades = preds.sum()
        
        print(f"Fold {fold+1}: Best Thresh={fold_best_thresh:.2f} | Precision={fold_best_prec:.2%} | Trades={fold_trades}")
        if fold_trades > 0:
            scores.append(fold_best_prec)
            best_thresholds.append(fold_best_thresh)

    avg_prec = np.mean(scores) if scores else 0
    avg_thresh = np.mean(best_thresholds) if best_thresholds else 0.70
    
    print(f"\nğŸ† Ù…ØªÙˆØ³Ø· Ø§Ù„Ø¯Ù‚Ø© Ø§Ù„Ù…ØªÙˆÙ‚Ø¹Ø©: {avg_prec:.2%}")
    print(f"ğŸ”‘ Ø§Ù„Ø¹ØªØ¨Ø© Ø§Ù„Ù…ÙˆØµÙ‰ Ø¨Ù‡Ø§: {avg_thresh:.2f}")

    # Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ
    print("ğŸš€ ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ...")
    final_ratio = np.sqrt((y==0).sum() / (y==1).sum())
    final_model = VotingClassifier(
        estimators=[
            ('xgb', XGBClassifier(n_estimators=500, max_depth=5, learning_rate=0.02, scale_pos_weight=final_ratio, n_jobs=-1)),
            ('cat', CatBoostClassifier(iterations=500, depth=5, learning_rate=0.02, auto_class_weights='SqrtBalanced', verbose=False)),
            ('lgbm', LGBMClassifier(n_estimators=500, max_depth=5, learning_rate=0.02, class_weight='balanced', verbose=-1))
        ], voting='soft'
    )
    final_model.fit(X, y)
    
    # Ø­ÙØ¸ Ø§Ù„Ø¹ØªØ¨Ø© Ù…Ø¹ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
    model_data = {'model': final_model, 'threshold': avg_thresh}
    if not os.path.exists('models'): os.makedirs('models')
    joblib.dump(model_data, 'models/btc_v9_worldclass.pkl')
    print("âœ… ØªÙ… Ø§Ù„Ø­ÙØ¸ (Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ + Ø§Ù„Ø¹ØªØ¨Ø© Ø§Ù„Ù…Ø«Ø§Ù„ÙŠØ©).")

if __name__ == "__main__":
    train_brain_v9()