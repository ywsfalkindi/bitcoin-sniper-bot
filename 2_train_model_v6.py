import pandas as pd
import numpy as np
import pandas_ta as ta
from xgboost import XGBClassifier
from sklearn.model_selection import TimeSeriesSplit
from sklearn.metrics import classification_report, roc_auc_score
import joblib
import os

def feature_engineering(df):
    """ Ù‡Ù†Ø¯Ø³Ø© Ø§Ù„Ù…ÙŠØ²Ø§Øª Ø§Ù„Ù…ØªÙ‚Ø¯Ù…Ø© (The Secret Sauce) """
    data = df.copy()
    
    # 1. Heikin Ashi (Ù„ØªÙ‚Ù„ÙŠÙ„ Ø§Ù„Ø¶ÙˆØ¶Ø§Ø¡)
    data['HA_Close'] = (data['open'] + data['high'] + data['low'] + data['close']) / 4
    data['HA_Open'] = (data['open'].shift(1) + data['close'].shift(1)) / 2
    data['HA_High'] = data[['high', 'HA_Open', 'HA_Close']].max(axis=1)
    data['HA_Low'] = data[['low', 'HA_Open', 'HA_Close']].min(axis=1)
    
    # 2. Ù…Ø¤Ø´Ø±Ø§Øª Ø§Ù„Ø²Ø®Ù… Ø§Ù„Ù…ØªÙ‚Ø¯Ù…Ø©
    data['RSI'] = data.ta.rsi(length=14)
    data['ADX'] = data.ta.adx(length=14)['ADX_14'] 
    
    # 3. Volatility (Garman-Klass)
    data['Log_Ret'] = np.log(data['close'] / data['close'].shift(1))
    data['Volatility'] = data['Log_Ret'].rolling(window=24).std() * np.sqrt(24)
    data['ATR'] = data.ta.atr(length=14)
    
    # 4. Z-Score
    data['Z_Score'] = (data['close'] - data['close'].rolling(20).mean()) / (data['close'].rolling(20).std() + 1e-9)
    
    # 5. Cyclical Time Features
    data['hour_sin'] = np.sin(2 * np.pi * data['timestamp'].dt.hour / 24)
    data['hour_cos'] = np.cos(2 * np.pi * data['timestamp'].dt.hour / 24)
    
    # 6. Proxy Order Flow
    data['Buying_Pressure'] = (data['close'] - data['open']) / (data['high'] - data['low'] + 1e-9) * data['volume']
    
    # 7. Context
    data['Trend_4H'] = (data['close_4h'] > data['close_4h'].shift(1)).astype(int)
    data['Divergence'] = data['close'] / data['close_4h']
    
    data.dropna(inplace=True)
    return data

def labeling_triple_barrier(df, atr_mult_tp=2.5, atr_mult_sl=1.0, horizon=12):
    """ Ù†Ø¸Ø§Ù… Ø§Ù„Ø£Ù‡Ø¯Ø§Ù Ø§Ù„Ø«Ù„Ø§Ø«ÙŠ """
    targets = []
    for i in range(len(df) - horizon):
        curr_close = df.iloc[i]['close']
        curr_atr = df.iloc[i]['ATR']
        
        tp = curr_close + (curr_atr * atr_mult_tp)
        sl = curr_close - (curr_atr * atr_mult_sl)
        
        future = df.iloc[i+1 : i+horizon+1]
        
        hit_tp = False
        hit_sl = False
        
        for _, row in future.iterrows():
            if row['low'] <= sl:
                hit_sl = True
                break
            if row['high'] >= tp:
                hit_tp = True
                break
                
        if hit_tp and not hit_sl:
            targets.append(1)
        else:
            targets.append(0)
            
    return targets

def train_brain_v6():
    print("ğŸ§  (V6) Ø¨Ø¯Ø¡ ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø¹Ø§Ù„Ù…ÙŠ...")
    
    if not os.path.exists('data/btc_data_v6.csv'):
        print("âŒ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù…ÙÙ‚ÙˆØ¯Ø©!")
        return

    df = pd.read_csv('data/btc_data_v6.csv')
    df['timestamp'] = pd.to_datetime(df['timestamp'])
    
    # 1. Ø§Ù„ØªØ¬Ù‡ÙŠØ²
    print("ğŸ› ï¸ Ù‡Ù†Ø¯Ø³Ø© Ø§Ù„Ù…ÙŠØ²Ø§Øª...")
    df = feature_engineering(df)
    
    print("ğŸ¯ ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ø£Ù‡Ø¯Ø§Ù...")
    targets = labeling_triple_barrier(df)
    
    df = df.iloc[:len(targets)]
    df['Target'] = targets
    
    features = [
        'RSI', 'ADX', 'Z_Score', 'Volatility', 
        'Buying_Pressure', 'fundingRate', 
        'hour_sin', 'hour_cos', 'Trend_4H', 'Divergence'
    ]
    
    X = df[features]
    y = df['Target']
    
    if len(X) < 100:
        print("âš ï¸ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù‚Ù„ÙŠÙ„Ø© Ø¬Ø¯Ø§Ù‹.")
        return

    # Ø­Ø³Ø§Ø¨ Ø§Ù„ÙˆØ²Ù†
    neg, pos = np.bincount(y)
    scale = neg / pos if pos > 0 else 1
    
    # Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ø´ØªØ±ÙƒØ©
    model_params = {
        'n_estimators': 2000,
        'learning_rate': 0.005,
        'max_depth': 6,
        'subsample': 0.7,
        'colsample_bytree': 0.7,
        'scale_pos_weight': scale,
        'objective': 'binary:logistic',
        'n_jobs': -1,
        'random_state': 42
    }

    # --- Ø§Ù„Ù…Ø±Ø­Ù„Ø© 1: Ø§Ù„ØªØ­Ù‚Ù‚ (Cross Validation) Ù…Ø¹ Ø§Ù„Ø¥ÙŠÙ‚Ø§Ù Ø§Ù„Ù…Ø¨ÙƒØ± ---
    print("ğŸ‹ï¸â€â™‚ï¸ Ø§Ù„ØªØ¯Ø±ÙŠØ¨ ÙˆØ§Ù„ØªØ­Ù‚Ù‚ (Cross-Validation)...")
    tscv = TimeSeriesSplit(n_splits=5)
    
    try:
        # Ù‡Ù†Ø§ Ù†Ø³ØªØ®Ø¯Ù… Ù†Ø³Ø®Ø© Ù…Ø¹ early_stopping_rounds
        for train_index, test_index in tscv.split(X):
            X_train, X_test = X.iloc[train_index], X.iloc[test_index]
            y_train, y_test = y.iloc[train_index], y.iloc[test_index]
            
            # Ù†Ø¹Ø±Ù Ù†Ù…ÙˆØ°Ø¬Ø§Ù‹ Ù…Ø¤Ù‚ØªØ§Ù‹ Ù„Ù„Ø§Ø®ØªØ¨Ø§Ø±
            cv_model = XGBClassifier(**model_params, early_stopping_rounds=50)
            cv_model.fit(X_train, y_train, eval_set=[(X_test, y_test)], verbose=False)
        
        print("   âœ… Ø§Ù†ØªÙ‡Ù‰ Ø§Ù„ØªØ­Ù‚Ù‚ Ø¨Ù†Ø¬Ø§Ø­.")

        # --- Ø§Ù„Ù…Ø±Ø­Ù„Ø© 2: Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ (Production) Ø¨Ø¯ÙˆÙ† Ø¥ÙŠÙ‚Ø§Ù Ù…Ø¨ÙƒØ± ---
        print("ğŸš€ Ø¬Ø§Ø±ÙŠ Ø¨Ù†Ø§Ø¡ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ Ø¹Ù„Ù‰ ÙƒØ§Ù…Ù„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª...")
        
        # Ù†Ø¹Ø±Ù Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ (Ø¨Ø¯ÙˆÙ† early_stopping_rounds ÙÙŠ Ø§Ù„Ù…ÙÙ†Ø´Ø¦)
        final_model = XGBClassifier(**model_params)
        
        # Ù†Ø¯Ø±Ø¨Ù‡ Ø¹Ù„Ù‰ ÙƒÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
        final_model.fit(X, y, verbose=False)
        
        if not os.path.exists('models'): os.makedirs('models')
        joblib.dump(final_model, 'models/btc_v6_model.pkl')
        
        print("\nğŸ“Š Ø£Ù‡Ù… Ø§Ù„Ø¹ÙˆØ§Ù…Ù„ Ø§Ù„Ù…Ø¤Ø«Ø±Ø©:")
        imps = pd.Series(final_model.feature_importances_, index=features).sort_values(ascending=False)
        print(imps.head(6))
        print("\nâœ… ØªÙ… Ø¨Ù†Ø§Ø¡ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ V6 Ø¨Ù†Ø¬Ø§Ø­ ÙˆØ¬Ø§Ù‡Ø² Ù„Ù„Ù‚Ù†Øµ!")
        
    except Exception as e:
        print(f"âŒ Ø­Ø¯Ø« Ø®Ø·Ø£ Ø£Ø«Ù†Ø§Ø¡ Ø§Ù„ØªØ¯Ø±ÙŠØ¨: {e}")

if __name__ == "__main__":
    train_brain_v6()